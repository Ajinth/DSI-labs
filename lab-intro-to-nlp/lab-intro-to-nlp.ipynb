{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to NLP Lab\n",
    "\n",
    "In this lab, you'll be classifying randomly selected tweets from political officials into whether or not they are partisan tweets or neutral. In the following import statement, we're selecting only the columns that are important, but there may be more useful features in that set. Feel free to explore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_predict, cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, \\\n",
    "HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import make_union, make_pipeline, Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>partisan</td>\n",
       "      <td>RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>partisan</td>\n",
       "      <td>VIDEO - #Obamacare:  Full of Higher Costs and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Please join me today in remembering our fallen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>RT @SenatorLeahy: 1st step toward Senate debat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>partisan</td>\n",
       "      <td>.@amazon delivery #drones show need to update ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bias                                               text\n",
       "0  partisan  RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...\n",
       "1  partisan  VIDEO - #Obamacare:  Full of Higher Costs and ...\n",
       "2   neutral  Please join me today in remembering our fallen...\n",
       "3   neutral  RT @SenatorLeahy: 1st step toward Senate debat...\n",
       "4  partisan  .@amazon delivery #drones show need to update ..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/political_media.csv',\n",
    "                usecols=[7, 20])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up\n",
    "\n",
    "Please split the dataset into a training and test set and convert the `bias` feature into 0s and 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bias'].unique()\n",
    "\n",
    "X = df['text'].values\n",
    "y = df['bias'].map(lambda x: 1 if x == 'partisan' else 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.values, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Please try the following techniques to transform the data. For each technique, do the following:\n",
    "\n",
    "1. Transform the training data\n",
    "2. Fit a `RandomForestClassifier` to the transformed training data\n",
    "3. Transform the test data\n",
    "4. Discuss the goodness of fit of your model using the test data and a classification report and confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. `CountVectorizer()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97223880597\n",
      "[[2474    1]\n",
      " [  92  783]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98      2475\n",
      "          1       1.00      0.89      0.94       875\n",
      "\n",
      "avg / total       0.97      0.97      0.97      3350\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Xtrain_cv = cv.fit_transform(X_train)\n",
    "Xtest_cv = cv.transform(X_test)\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(Xtrain_cv, y_train)\n",
    "print(rfc.score(Xtrain_cv, y_train))\n",
    "print(confusion_matrix(y_train, rfc.predict(Xtrain_cv)))\n",
    "print(classification_report(y_train, rfc.predict(Xtrain_cv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1182   32]\n",
      " [ 386   50]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.97      0.85      1214\n",
      "          1       0.61      0.11      0.19       436\n",
      "\n",
      "avg / total       0.72      0.75      0.68      1650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc.score(Xtest_cv, y_test)\n",
    "print(confusion_matrix(y_test, rfc.predict(Xtest_cv)))\n",
    "print(classification_report(y_test, rfc.predict(Xtest_cv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. `CountVectorizer()` with your choice of `min_df` and `max_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97671641791\n",
      "[[2470    5]\n",
      " [  73  802]]\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(min_df=0.01, max_df=0.80)\n",
    "cv.fit(X_train)\n",
    "\n",
    "X_train_cv = cv.transform(X_train)\n",
    "X_test_cv = cv.transform(X_test)\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train_cv, y_train)\n",
    "print(rfc.score(X_train_cv, y_train))\n",
    "print(confusion_matrix(y_train, rfc.predict(X_train_cv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98      2475\n",
      "          1       0.99      0.92      0.95       875\n",
      "\n",
      "avg / total       0.98      0.98      0.98      3350\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, rfc.predict(X_train_cv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.760606060606\n",
      "[[1157   57]\n",
      " [ 338   98]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.95      0.85      1214\n",
      "          1       0.63      0.22      0.33       436\n",
      "\n",
      "avg / total       0.74      0.76      0.72      1650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rfc.score(X_test_cv, y_test))\n",
    "print(confusion_matrix(y_test, rfc.predict(X_test_cv)))\n",
    "print(classification_report(y_test, rfc.predict(X_test_cv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. `CountVectorizer()` with English stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "cv.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.975820895522\n",
      "[[2474    1]\n",
      " [  80  795]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98      2475\n",
      "          1       1.00      0.91      0.95       875\n",
      "\n",
      "avg / total       0.98      0.98      0.98      3350\n",
      "\n",
      "---------------------------------------------------\n",
      "0.73696969697\n",
      "[[1151   63]\n",
      " [ 371   65]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.95      0.84      1214\n",
      "          1       0.51      0.15      0.23       436\n",
      "\n",
      "avg / total       0.69      0.74      0.68      1650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_cv = cv.transform(X_train)\n",
    "X_test_cv = cv.transform(X_test)\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train_cv, y_train)\n",
    "print(rfc.score(X_train_cv, y_train))\n",
    "print(confusion_matrix(y_train, rfc.predict(X_train_cv)))\n",
    "print(classification_report(y_train, rfc.predict(X_train_cv)))\n",
    "print('---------------------------------------------------')\n",
    "print(rfc.score(X_test_cv, y_test))\n",
    "print(confusion_matrix(y_test, rfc.predict(X_test_cv)))\n",
    "print(classification_report(y_test, rfc.predict(X_test_cv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. `TfidfVectorizer()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv = TfidfVectorizer()\n",
    "tv.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.721818181818\n",
      "[[1054  160]\n",
      " [ 299  137]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.87      0.82      1214\n",
      "          1       0.46      0.31      0.37       436\n",
      "\n",
      "avg / total       0.70      0.72      0.70      1650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Xtrain_tv = tv.transform(X_train)\n",
    "Xtest_tv = tv.transform(X_test)\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(Xtrain_tv, y_train)\n",
    "print(rfc.score(Xtest_cv, y_test))\n",
    "print(confusion_matrix(y_test, rfc.predict(Xtest_cv)))\n",
    "print(classification_report(y_test, rfc.predict(Xtest_cv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. `TfidfVectorizer()` with English stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=0.01,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv = TfidfVectorizer(stop_words='english', min_df=0.01)\n",
    "tv.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.741818181818\n",
      "[[1130   84]\n",
      " [ 342   94]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.93      0.84      1214\n",
      "          1       0.53      0.22      0.31       436\n",
      "\n",
      "avg / total       0.70      0.74      0.70      1650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Xtrain_tv = tv.transform(X_train)\n",
    "Xtest_tv = tv.transform(X_test)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(Xtrain_tv, y_train)\n",
    "print(rf.score(Xtest_tv, y_test))\n",
    "print(confusion_matrix(y_test, rf.predict(Xtest_tv)))\n",
    "print(classification_report(y_test, rf.predict(Xtest_tv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving forward\n",
    "\n",
    "With the remainder of your time, please try and find the best model and data transformation to predict partisan tweets. This is a challenging data set and can be approached from a number of ways.\n",
    "\n",
    "Some techniques to try are:\n",
    "\n",
    "1. Different types of data transformation \n",
    "2. Custom preprocessors for `CountVectorizer`\n",
    "3. Custom stopword lists\n",
    "4. Use of a dimensionality reduction technique (like `TruncatedSVD`)\n",
    "5. Optimizing hyperparameters using `GridSearchCV`\n",
    "6. Trying a different modeling technique such as `KNeighborsClassifier` or `LogisticRegression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bias'].unique()\n",
    "\n",
    "X = df['text'].values\n",
    "y = df['bias'].map(lambda x: 1 if x == 'partisan' else 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.values, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.962089552239\n",
      "[[2442   14]\n",
      " [ 113  781]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.99      0.97      2456\n",
      "          1       0.98      0.87      0.92       894\n",
      "\n",
      "avg / total       0.96      0.96      0.96      3350\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________\n",
      "Testing data\n",
      "0.700606060606\n",
      "[[1084  149]\n",
      " [ 345   72]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.88      0.81      1233\n",
      "          1       0.33      0.17      0.23       417\n",
      "\n",
      "avg / total       0.65      0.70      0.67      1650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "cv.fit(X_train)\n",
    "Xtrain_cv = cv.transform(X_train)\n",
    "tsvd = TruncatedSVD()\n",
    "\n",
    "Xtrain_tsvd = tsvd.fit_transform(Xtrain_cv)\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(Xtrain_tsvd, y_train)\n",
    "print(rfc.score(Xtrain_tsvd, y_train))\n",
    "print(confusion_matrix(y_train, rfc.predict(Xtrain_tsvd)))\n",
    "print(classification_report(y_train, rfc.predict(Xtrain_tsvd)))\n",
    "print('\\n')\n",
    "print('_______________________________________________________')\n",
    "print('Testing data')\n",
    "\n",
    "Xtest_cv = cv.transform(X_test)\n",
    "Xtest_tsvd = tsvd.transform(Xtest_cv)\n",
    "print(rfc.score(Xtest_tsvd, y_test))\n",
    "print(confusion_matrix(y_test, rfc.predict(Xtest_tsvd)))\n",
    "print(classification_report(y_test, rfc.predict(Xtest_tsvd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaner function\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "def cleaner(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stop = stopwords.words('english')\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.translate(str.maketrans('', '', string.digits))\n",
    "    text = text.lower().strip()\n",
    "    final_text = []\n",
    "    for w in text.split():\n",
    "        if w not in stop:\n",
    "            final_text.append(stemmer.stem(w.strip()))\n",
    "    return ' '.join(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.977313432836\n",
      "[[2454    2]\n",
      " [  74  820]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98      2456\n",
      "          1       1.00      0.92      0.96       894\n",
      "\n",
      "avg / total       0.98      0.98      0.98      3350\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________\n",
      "Testing data\n",
      "0.733333333333\n",
      "[[1175   58]\n",
      " [ 382   35]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.95      0.84      1233\n",
      "          1       0.38      0.08      0.14       417\n",
      "\n",
      "avg / total       0.66      0.73      0.66      1650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(preprocessor=cleaner)\n",
    "cv.fit(X_train)\n",
    "Xtrain_cv = cv.transform(X_train)\n",
    "tsvd = TruncatedSVD(n_components=500)\n",
    "\n",
    "Xtrain_tsvd = tsvd.fit_transform(Xtrain_cv)\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(Xtrain_tsvd, y_train)\n",
    "print(rfc.score(Xtrain_tsvd, y_train))\n",
    "print(confusion_matrix(y_train, rfc.predict(Xtrain_tsvd)))\n",
    "print(classification_report(y_train, rfc.predict(Xtrain_tsvd)))\n",
    "print('\\n')\n",
    "print('_______________________________________________________')\n",
    "print('Testing data')\n",
    "\n",
    "Xtest_cv = cv.transform(X_test)\n",
    "Xtest_tsvd = tsvd.transform(Xtest_cv)\n",
    "print(rfc.score(Xtest_tsvd, y_test))\n",
    "print(confusion_matrix(y_test, rfc.predict(Xtest_tsvd)))\n",
    "print(classification_report(y_test, rfc.predict(Xtest_tsvd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3350,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3350,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = TfidfVectorizer(preprocessor=cleaner)\n",
    "tf.fit(X_train)\n",
    "politics = pd.DataFrame(tf.transform(X_train).todense(), columns=tf.get_feature_names())\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "today       58.773242\n",
       "űş          46.562544\n",
       "amp         41.926708\n",
       "great       40.306478\n",
       "hous        38.012409\n",
       "thank       36.470600\n",
       "job         35.953632\n",
       "work        35.094587\n",
       "american    34.742684\n",
       "us          34.397409\n",
       "dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politics.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_words = ['today', 'amp', 'great', 'hous', 'thank', 'job', 'work', 'us', 'american']\n",
    "stop = stopwords.words('english')\n",
    "stop.extend(custom_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.980298507463\n",
      "[[2453    3]\n",
      " [  63  831]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99      2456\n",
      "          1       1.00      0.93      0.96       894\n",
      "\n",
      "avg / total       0.98      0.98      0.98      3350\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________________\n",
      "Testing data\n",
      "0.739393939394\n",
      "[[1164   69]\n",
      " [ 361   56]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.94      0.84      1233\n",
      "          1       0.45      0.13      0.21       417\n",
      "\n",
      "avg / total       0.68      0.74      0.68      1650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(preprocessor=cleaner, stop_words=stop)\n",
    "cv.fit(X_train)\n",
    "Xtrain_cv = cv.transform(X_train)\n",
    "tsvd = TruncatedSVD(n_components=400)\n",
    "\n",
    "Xtrain_tsvd = tsvd.fit_transform(Xtrain_cv)\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(Xtrain_tsvd, y_train)\n",
    "print(rfc.score(Xtrain_tsvd, y_train))\n",
    "print(confusion_matrix(y_train, rfc.predict(Xtrain_tsvd)))\n",
    "print(classification_report(y_train, rfc.predict(Xtrain_tsvd)))\n",
    "print('\\n')\n",
    "print('_______________________________________________________')\n",
    "print('Testing data')\n",
    "\n",
    "Xtest_cv = cv.transform(X_test)\n",
    "Xtest_tsvd = tsvd.transform(Xtest_cv)\n",
    "print(rfc.score(Xtest_tsvd, y_test))\n",
    "print(confusion_matrix(y_test, rfc.predict(Xtest_tsvd)))\n",
    "print(classification_report(y_test, rfc.predict(Xtest_tsvd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying now with Gridsearch and pipeline steps, with 3 classifier models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the last hyperparameters didn't work as well and didn't provide good recall on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "log_pipe = Pipeline([('tfvec', TfidfVectorizer(preprocessor=cleaner, stop_words=stop)),\n",
    "                     ('tsvd', TruncatedSVD(n_components=400)),\n",
    "                    ('lg', LogisticRegression(penalty='l1', C=0.5))])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfvec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=<function cleane...ty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76507462686567163"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "log_pipe = Pipeline([('tfvec', TfidfVectorizer(preprocessor=cleaner, stop_words=stop)),\n",
    "                     ('tsvd', TruncatedSVD(n_components=400)),\n",
    "                    ('lg', KNeighborsClassifier(n_neighbors=3))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfvec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=<function cleane...owski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "           weights='uniform'))])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84268656716417911"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_pipe.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66000000000000003"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[945 288]\n",
      " [273 144]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, log_pipe.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.77      0.77      1233\n",
      "          1       0.33      0.35      0.34       417\n",
      "\n",
      "avg / total       0.66      0.66      0.66      1650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, log_pipe.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "cv = CountVectorizer(preprocessor=cleaner, stop_words=stop)\n",
    "Xtrain_cv = cv.fit_transform(X_train)\n",
    "tsvd = TruncatedSVD(n_components=500)\n",
    "Xtrain_tsvd = tsvd.fit_transform(Xtrain_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'n_estimators':[5,6,7,8,9,10],\n",
    "    'criterion': ['gini', 'entropy']}\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "gs = GridSearchCV(rfc,param_grid=grid,n_jobs=-1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:    4.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': [5, 6, 7, 8, 9, 10], 'criterion': ['gini', 'entropy']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(Xtrain_tsvd, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsb = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy', 'n_estimators': 6}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest_cv = cv.transform(X_test)\n",
    "Xtest_tsvd = tsvd.transform(Xtest_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74242424242424243"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsb.score(Xtest_tsvd, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
